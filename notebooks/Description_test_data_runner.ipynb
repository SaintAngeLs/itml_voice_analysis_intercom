{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f841431-cefd-4b48-b72e-7130e85687c4",
   "metadata": {},
   "source": [
    "Sure! Below is a detailed Markdown template you can use in a Jupyter notebook to describe your solution.\n",
    "\n",
    "---\n",
    "\n",
    "# Voice-Based Door Access Recognition System\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project is part of the course **Introduction to Machine Learning (2024/2025)** by **Dr. Agnieszka Jastrzębska**. The aim of this project is to develop a **voice recognition system** for an automated intercom, capable of distinguishing between allowed and disallowed persons using voice recordings. The core idea is to convert voice recordings into spectrograms and apply **Convolutional Neural Networks (CNNs)** for classification.\n",
    "\n",
    "This is a binary classification problem, where:\n",
    "- **Class 1 (allowed)**: People allowed to open the door.\n",
    "- **Class 0 (disallowed)**: People not allowed to open the door.\n",
    "\n",
    "The data used for the project is derived from the **DAPS (Device and Produced Speech) Dataset**.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Project Structure](#project-structure)\n",
    "2. [Data Preprocessing](#data-preprocessing)\n",
    "3. [Model Architecture](#model-architecture)\n",
    "4. [Model Training](#model-training)\n",
    "5. [Model Evaluation](#model-evaluation)\n",
    "6. [Future Improvements](#future-improvements)\n",
    "7. [Conclusion](#conclusion)\n",
    "\n",
    "---\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "The project is organized as follows:\n",
    "\n",
    "```bash\n",
    ".\n",
    "├── data                     # Contains raw audio and generated spectrograms\n",
    "│   ├── allowed              # Audio files for Class 1 (allowed)\n",
    "│   ├── disallowed           # Audio files for Class 0 (disallowed)\n",
    "│   ├── spectrograms         # Generated spectrograms for training the model\n",
    "│   └── test_voices          # Test audio files\n",
    "├── models                   # Stores trained CNN models\n",
    "├── notebooks                # Jupyter notebooks for analysis and interaction\n",
    "├── src                      # Python scripts for data processing, training, and evaluation\n",
    "│   ├── data_preprocessing.py # Script for loading and generating spectrograms\n",
    "│   ├── model.py             # CNN model definition\n",
    "│   ├── train.py             # Script to train the CNN model\n",
    "│   └── evaluate.py          # Script to evaluate the model\n",
    "├── tests                    # Unit tests for different components\n",
    "└── requirements.txt         # Python dependencies\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "The input to our machine learning model is not the raw audio file but the **Mel-spectrogram**, a visual representation of the frequency spectrum. The steps involved in preprocessing are:\n",
    "\n",
    "1. **Load Audio**: Each `.mp3` or `.wav` file is loaded using the `librosa` library.\n",
    "2. **Generate Spectrograms**: Mel-spectrograms are generated using `librosa.feature.melspectrogram`. These spectrograms are saved as images (`.png`) for both classes (allowed and disallowed).\n",
    "3. **Output Structure**: The spectrograms are saved in directories labeled according to their respective classes (`allowed`, `disallowed`).\n",
    "\n",
    "### Code Example:\n",
    "\n",
    "```python\n",
    "# Load and preprocess the audio data\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def load_audio(audio_path, sr=22050):\n",
    "    y, sr = librosa.load(audio_path, sr=sr)\n",
    "    return y, sr\n",
    "\n",
    "def generate_spectrogram(audio_path, output_dir, sr=22050, n_mels=128):\n",
    "    y, sr = load_audio(audio_path, sr)\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "    log_S = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    output_file = os.path.join(output_dir, os.path.basename(audio_path).replace('.mp3', '.png'))\n",
    "    return log_S, output_file\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "We use a **Convolutional Neural Network (CNN)** to classify the spectrogram images into two categories: allowed or disallowed. The architecture is simple but effective, consisting of:\n",
    "\n",
    "1. **Three Convolutional Layers**: These layers extract spatial features from the spectrograms.\n",
    "2. **Max Pooling Layers**: Reduces the spatial dimensions to focus on the most critical parts of the image.\n",
    "3. **Dense Layer**: Fully connected layer for classification.\n",
    "4. **Dropout Layer**: Helps reduce overfitting by randomly dropping units during training.\n",
    "5. **Output Layer**: A single neuron activated by the sigmoid function to predict whether a person is allowed or not.\n",
    "\n",
    "### Model Summary:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_cnn_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Model Training\n",
    "\n",
    "The training process involves:\n",
    "\n",
    "- **Data Augmentation**: We use techniques such as rotation and zoom to artificially increase the size of our dataset and introduce variability.\n",
    "- **Train/Test Split**: The dataset is split into 80% training and 20% validation using `train_test_split`.\n",
    "- **Epochs**: The model is trained over 20 epochs using augmented data.\n",
    "\n",
    "### Training Code:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "datagen = ImageDataGenerator(rotation_range=10, zoom_range=0.1, horizontal_flip=True)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=32), epochs=20, validation_data=(X_val, y_val))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "1. **False Acceptance Ratio (FAR)**: Measures how often a disallowed person is incorrectly accepted.\n",
    "2. **False Rejection Ratio (FRR)**: Measures how often an allowed person is incorrectly rejected.\n",
    "\n",
    "The evaluation process includes:\n",
    "\n",
    "- **Predicting on Test Set**: The model's performance is evaluated on unseen test data.\n",
    "- **Calculating FAR/FRR**: Using a confusion matrix to calculate these critical metrics.\n",
    "\n",
    "### Evaluation Code:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calculate_far_frr(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    far = fp / (fp + tn)\n",
    "    frr = fn / (fn + tp)\n",
    "    return far, frr\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Future Improvements\n",
    "\n",
    "Some ideas for further improvements:\n",
    "\n",
    "1. **Enhance Data Augmentation**: Apply more sophisticated augmentations to increase model robustness.\n",
    "2. **Advanced CNN Architectures**: Implement more advanced architectures like ResNet or VGG to improve performance.\n",
    "3. **Noise Handling**: Explore techniques for handling background noise more effectively, such as noise reduction or filtering.\n",
    "4. **Hyperparameter Tuning**: Experiment with different optimizers, learning rates, and batch sizes to improve model accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project demonstrates the application of **Convolutional Neural Networks** for voice recognition in a door-access system. By converting voice recordings into spectrograms, we leverage image-based recognition techniques to classify users based on their voice. The system shows promise and can be further improved with more sophisticated models and techniques.\n",
    "\n",
    "---\n",
    "\n",
    "This Markdown structure should give you a clear and professional presentation of your project when used in a Jupyter notebook. You can modify sections and details as necessary based on your specific work and findings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d5cc8-0fd1-48fc-a816-5f7ce4417510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
