{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f841431-cefd-4b48-b72e-7130e85687c4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Voice-Based Door Access Recognition System\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project is part of the course **Introduction to Machine Learning (2024/2025)** by **Dr. Agnieszka JastrzÄ™bska**. The aim of this project is to develop a **voice recognition system** for an automated intercom, capable of distinguishing between allowed and disallowed persons using voice recordings. The core idea is to convert voice recordings into spectrograms and apply **Convolutional Neural Networks (CNNs)** for classification.\n",
    "\n",
    "This is a binary classification problem, where:\n",
    "- **Class 1 (allowed)**: People allowed to open the door.\n",
    "- **Class 0 (disallowed)**: People not allowed to open the door.\n",
    "\n",
    "The data used for the project is derived from the **DAPS (Device and Produced Speech) Dataset**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Data Preprocessing\n",
    "The input to our machine learning model is not the raw audio file but the **Mel-spectrogram**, a visual representation of the frequency spectrum. The steps involved in preprocessing are:\n",
    "\n",
    "1. **Load Audio**: Each `.mp3` or `.wav` file is loaded using the `librosa` library. It is possible to apply random gain (volume change) to the audio: \n",
    "\n",
    "\n",
    "\n",
    "Silent sections are removed and the file is split in chunks if longer than max_duration. \n",
    "\n",
    "\n",
    "\n",
    "2. **Generate Spectrograms**: Mel-spectrograms are generated using `librosa.feature.melspectrogram`. These spectrograms are saved as images (`.png`) for both classes (allowed and disallowed).\n",
    "\n",
    "#### Code example:\n",
    "\n",
    "```python \n",
    "def generate_spectrogram(audio_data, sr, output_dir, file_name, segment_idx):\n",
    "    try:\n",
    "        # Create mel-spectrogram and convert to decibel scale\n",
    "        S = librosa.feature.melspectrogram(y=audio_data, sr=sr, n_mels=128)\n",
    "        log_S = librosa.power_to_db(S, ref=np.max)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating spectrogram for {file_name} segment {segment_idx}: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    output_file = os.path.join(output_dir, f\"{file_name}_segment_{segment_idx}_spectrogram.png\")\n",
    "    try:\n",
    "        # Plot and save the spectrogram\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel', fmax=8000)\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Mel-frequency spectrogram')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_file)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving spectrogram for {file_name} segment {segment_idx}: {e}\")\n",
    "        return None\n",
    "    return output_file\n",
    "```\n",
    "\n",
    "3. **Output Structure**: The spectrograms are saved in directories labeled according to their respective classes (`allowed`, `disallowed`).\n",
    "\n",
    "---\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "We use a **Convolutional Neural Network (CNN)** to classify the spectrogram images into two categories: allowed or disallowed. The architecture is simple but effective, consisting of:\n",
    "\n",
    "1. **Initial Convolutional Layer**: A convolutional layer with $64$ filters of size $3 \\times 3$ is applied, followed by batch normalization and the ReLU activation function.  \n",
    "2. **Three Sets of Convolutional Layers**: Each set consists of two convolutional operations with increasing filter sizes (64, 128, and 256). These layers extract progressively abstract features from the input spectrogram. \n",
    "3. **Max Pooling**: Reduces the spatial dimensions to focus on the most critical parts of the image. It is applied between each set of convolutions.\n",
    "4. **Global Average Pooling**: When applied, it reduces each feature map into a single scalar by averaging over all spatial locations.\n",
    "5. **Dense Layer**: Fully connected layer for classification.\n",
    "4. **Dropout Layer**: Helps reduce overfitting by randomly dropping units during training.\n",
    "5. **Output Layer**: A single neuron activated by the sigmoid function to predict whether a person is allowed access.\n",
    "\n",
    "---\n",
    "\n",
    "## Model Training\n",
    "\n",
    "The training process involves:\n",
    "- **Data Augmentation**: We use techniques such as rotation, zoom and horizontal flip to artificially increase the size of our dataset and introduce variability.\n",
    "- **Class balancing**: There are fewer allowed speakers than disallowed, which is why class weights $w_c$ are applied during training to adjust the loss for each class.\n",
    "- **Optimization using the Adam algorithm**: The Adaptive Moment Estimation optimizer is used to minimize the loss function.\n",
    "- **Train/Test Split**: The dataset is split into 80% training and 20% validation using `train_test_split`.\n",
    "- **Early stopping**: The model's performance is monitored on the validation set, and training is stopped if the validation loss does not improve after $p = 15$ epochs.\n",
    "\n",
    "---\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "1. **False Acceptance Ratio (FAR)**: Measures how often a disallowed person is incorrectly accepted.\n",
    "2. **False Rejection Ratio (FRR)**: Measures how often an allowed person is incorrectly rejected.\n",
    "3. **General Efficiency Coefficient**: A comprehensive metric that combines the model's accuracy, FAR, and FRR. The GEC is defined as:\n",
    "\n",
    "    $\\text{GEC} = w_1 \\cdot \\text{Accuracy} + w_2 \\cdot (1 - \\text{FAR}) + w_3 \\cdot (1 - \\text{FRR})$\n",
    "\n",
    "    where:\n",
    "    - $\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$\n",
    "    - $w_1 = 0.4$, $w_2 = 0.3$, and $w_3 = 0.3$ are the weights assigned to accuracy, $(1 - \\text{FAR})$, and $(1 - \\text{FRR})$ respectively, reflecting their relative importance in the overall evaluation.\n",
    "\n",
    "### FAR/FRR Calculation Code:\n",
    "\n",
    "```python\n",
    "def calculate_far_frr_binary(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate False Acceptance Ratio (FAR) and False Rejection Ratio (FRR) for binary classification.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Ground truth labels (1 for allowed, 0 for disallowed)\n",
    "    - y_pred: Model predictions (1 for allowed, 0 for disallowed)\n",
    "    \n",
    "    Returns:\n",
    "    - FAR: False Acceptance Ratio (disallowed incorrectly classified as allowed)\n",
    "    - FRR: False Rejection Ratio (allowed incorrectly classified as disallowed)\n",
    "    - tn: True negatives count\n",
    "    - fp: False positives count\n",
    "    - fn: False negatives count\n",
    "    - tp: True positives count\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    far = fp / (fp + tn) if (fp + tn) != 0 else 0  # False Acceptance Ratio\n",
    "    frr = fn / (fn + tp) if (fn + tp) != 0 else 0  # False Rejection Ratio\n",
    "\n",
    "    return far, frr, tn, fp, fn, tp\n",
    "```\n",
    "\n",
    "### GEC Calcultion Code: \n",
    "```python\n",
    "def calculate_general_efficiency_coefficient(accuracy, far, frr, w1=0.4, w2=0.3, w3=0.3):\n",
    "\"\"\"\n",
    "Calculate the General Efficiency Coefficient (GEC) as a weighted average of accuracy, (1 - FAR), and (1 - FRR).\n",
    "\n",
    "Parameters:\n",
    "- accuracy: Overall accuracy of the model\n",
    "- far: False Acceptance Ratio\n",
    "- frr: False Rejection Ratio\n",
    "- w1, w2, w3: Weights for accuracy, (1 - FAR), and (1 - FRR), respectively\n",
    "\n",
    "Returns:\n",
    "- gec: Calculated General Efficiency Coefficient\n",
    "\"\"\"\n",
    "gec = w1 * accuracy + w2 * (1 - far) + w3 * (1 - frr)\n",
    "return gec\n",
    "```\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "## Future Improvements  --- Where are they? \n",
    "\n",
    "Some ideas for further improvements:\n",
    "\n",
    "1. **Enhance Data Augmentation**: Apply more sophisticated augmentations to increase model robustness.\n",
    "2. **Advanced CNN Architectures**: Implement more advanced architectures like ResNet or VGG to improve performance.\n",
    "3. **Noise Handling**: Explore techniques for handling background noise more effectively, such as noise reduction or filtering.\n",
    "4. **Hyperparameter Tuning**: Experiment with different optimizers, learning rates, and batch sizes to improve model accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project demonstrates the application of **Convolutional Neural Networks** for voice recognition in a door-access system. By converting voice recordings into spectrograms, we leverage image-based recognition techniques to classify users based on their voice. The system shows promise and can be further improved with more sophisticated models and techniques.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d5cc8-0fd1-48fc-a816-5f7ce4417510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
