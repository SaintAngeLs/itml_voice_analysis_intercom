{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f841431-cefd-4b48-b72e-7130e85687c4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Voice-Based Door Access Recognition System\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project is part of the course **Introduction to Machine Learning (2024/2025)** by **Dr. Agnieszka Jastrzębska**. The aim of this project is to develop a **voice recognition system** for an automated intercom, capable of distinguishing between allowed and disallowed persons using voice recordings. The core idea is to convert voice recordings into spectrograms and apply **Convolutional Neural Networks (CNNs)** for classification.\n",
    "\n",
    "This is a binary classification problem, where:\n",
    "- **Class 1 (allowed)**: People allowed to open the door.\n",
    "- **Class 0 (disallowed)**: People not allowed to open the door.\n",
    "\n",
    "The data used for the project is derived from the **DAPS (Device and Produced Speech) Dataset**.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Project Structure](#project-structure)\n",
    "2. [Data Preprocessing](#data-preprocessing)\n",
    "3. [Model Architecture](#model-architecture)\n",
    "4. [Model Training](#model-training)\n",
    "5. [Model Evaluation](#model-evaluation)\n",
    "6. [Future Improvements](#future-improvements)\n",
    "7. [Conclusion](#conclusion)\n",
    "\n",
    "---\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "The project is organized as follows:\n",
    "\n",
    "```bash\n",
    ".\n",
    "├── data                     # Contains raw audio and generated spectrograms\n",
    "│   ├── allowed              # Audio files for Class 1 (allowed)\n",
    "│   ├── disallowed           # Audio files for Class 0 (disallowed)\n",
    "│   ├── spectrograms         # Generated spectrograms for training the model\n",
    "│   └── test_voices          # Test audio files\n",
    "├── models                   # Stores trained CNN models\n",
    "├── notebooks                # Jupyter notebooks for analysis and interaction\n",
    "├── src                      # Python scripts for data processing, training, and evaluation\n",
    "│   ├── data_preprocessing.py # Script for loading and generating spectrograms\n",
    "│   ├── model.py             # CNN model definition\n",
    "│   ├── train.py             # Script to train the CNN model\n",
    "│   └── evaluate.py          # Script to evaluate the model\n",
    "├── tests                    # Unit tests for different components\n",
    "└── requirements.txt         # Python dependencies\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Data Preprocessing\n",
    "The input to our machine learning model is not the raw audio file but the **Mel-spectrogram**, a visual representation of the frequency spectrum. The steps involved in preprocessing are:\n",
    "\n",
    "1. **Load Audio**: Each `.mp3` or `.wav` file is loaded using the `librosa` library. It is possible to apply random gain (volume change) to the audio: \n",
    "\n",
    "#### Code example:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # For non-GUI rendering\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "def augment_audio(y, sr):\n",
    "    if np.random.rand() > 0.5: # 50% chance to apply gain augmentation\n",
    "        gain = np.random.uniform(0.7, 1.3) # Random gain factor between 0.7 and 1.3\n",
    "        y = y * gain # Apply gain\n",
    "    return y\n",
    "```\n",
    "\n",
    "\n",
    "Silent sections are removed and the file is split in chunks if longer than max_duration. \n",
    "\n",
    "#### Code example:\n",
    "\n",
    "```python\n",
    "def load_audio(audio_path, sr=22050, max_duration=120, top_db=30, augment=False):\n",
    "\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=sr)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {audio_path}: {e}\")\n",
    "        return [], sr\n",
    "\n",
    "    # Remove silent parts of the audio\n",
    "    non_silent_intervals = librosa.effects.split(y, top_db=top_db)\n",
    "    if len(non_silent_intervals) == 0:\n",
    "        print(f\"No non-silent intervals found in {audio_path}. Skipping.\")\n",
    "        return [], sr\n",
    "    y_nonsilent = np.concatenate([y[start:end] for start, end in non_silent_intervals])\n",
    "\n",
    "    # Augment the audio if enabled\n",
    "    if augment:\n",
    "        y_nonsilent = augment_audio(y_nonsilent, sr)\n",
    "\n",
    "    total_duration = len(y_nonsilent) / sr\n",
    "    if total_duration > max_duration:\n",
    "        # Split audio into chunks\n",
    "        max_samples = int(max_duration * sr)\n",
    "        audio_chunks = [y_nonsilent[i:i + max_samples] for i in range(0, len(y_nonsilent), max_samples)]\n",
    "    else:\n",
    "        # Return entire audio as a single chunk if within max_duration\n",
    "        audio_chunks = [y_nonsilent]\n",
    "    \n",
    "    return audio_chunks, sr\n",
    "\n",
    "```\n",
    "\n",
    "2. **Generate Spectrograms**: Mel-spectrograms are generated using `librosa.feature.melspectrogram`. These spectrograms are saved as images (`.png`) for both classes (allowed and disallowed).\n",
    "\n",
    "#### Code example:\n",
    "\n",
    "```python \n",
    "def generate_spectrogram(audio_data, sr, output_dir, file_name, segment_idx):\n",
    "    try:\n",
    "        # Create mel-spectrogram and convert to decibel scale\n",
    "        S = librosa.feature.melspectrogram(y=audio_data, sr=sr, n_mels=128)\n",
    "        log_S = librosa.power_to_db(S, ref=np.max)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating spectrogram for {file_name} segment {segment_idx}: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    output_file = os.path.join(output_dir, f\"{file_name}_segment_{segment_idx}_spectrogram.png\")\n",
    "    try:\n",
    "        # Plot and save the spectrogram\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel', fmax=8000)\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Mel-frequency spectrogram')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_file)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving spectrogram for {file_name} segment {segment_idx}: {e}\")\n",
    "        return None\n",
    "    return output_file\n",
    "```\n",
    "\n",
    "3. **Output Structure**: The spectrograms are saved in directories labeled according to their respective classes (`allowed`, `disallowed`).\n",
    "\n",
    "---\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "We use a **Convolutional Neural Network (CNN)** to classify the spectrogram images into two categories: allowed or disallowed. The architecture is simple but effective, consisting of:\n",
    "\n",
    "1. **Initial Convolutional Layer**: A convolutional layer with $64$ filters of size $3 \\times 3$ is applied, followed by batch normalization and the ReLU activation function.  \n",
    "2. **Three Sets of Convolutional Layers**: Each set consists of two convolutional operations with increasing filter sizes (64, 128, and 256). These layers extract progressively abstract features from the input spectrogram. \n",
    "3. **Max Pooling**: Reduces the spatial dimensions to focus on the most critical parts of the image. It is applied between each set of convolutions.\n",
    "4. **Global Average Pooling**: When applied, it reduces each feature map into a single scalar by averaging over all spatial locations.\n",
    "5. **Dense Layer**: Fully connected layer for classification.\n",
    "4. **Dropout Layer**: Helps reduce overfitting by randomly dropping units during training.\n",
    "5. **Output Layer**: A single neuron activated by the sigmoid function to predict whether a person is allowed access.\n",
    "\n",
    "### Convolutional Layers:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, BatchNormalization, Dense, Input, Add, GlobalAveragePooling2D, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def residual_block(x, filters, kernel_size=(3, 3)):\n",
    "    \"\"\"Residual block with two convolutional layers.\"\"\"\n",
    "    shortcut = x  # Save the input tensor\n",
    "\n",
    "    # First convolutional layer\n",
    "    x = Conv2D(filters, kernel_size, padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x) # Apply batch normalization to improve training stability\n",
    "    x = Activation('relu')(x) # Use ReLU activation for non-linearity\n",
    "\n",
    "    # Second convolutional layer\n",
    "    x = Conv2D(filters, kernel_size, padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)  # Apply batch normalization again\n",
    "\n",
    "    # Adjust shortcut if needed\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv2D(filters, (1, 1), padding='same')(shortcut)  # Match the dimensions\n",
    "\n",
    "    # Add the shortcut to the output\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x) # Apply ReLU activation to the combined output\n",
    "    \n",
    "    return x # Return the output of the residual block\n",
    "```\n",
    "\n",
    "### Model creation: \n",
    "\n",
    "```python\n",
    "\n",
    "def create_cnn_model(input_shape):\n",
    "    \"\"\"Create a CNN model with residual blocks.\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Initial convolutional block\n",
    "    x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(0.001))(inputs)  # Initial convolution with 64 filters\n",
    "    x = BatchNormalization()(x) #Normalize the output\n",
    "    x = Activation('relu')(x) #apply ReLu activation\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x) # Downsample the feature maps\n",
    "\n",
    "    # Residual block 1\n",
    "    x = residual_block(x, 64) # Apply the first residual block with 64 filters\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x) # Downsample again\n",
    "\n",
    "    # Residual block 2\n",
    "    x = residual_block(x, 128) # Apply the second residual block with 128 filters\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x) # Downsample again\n",
    "\n",
    "    # Residual block 3\n",
    "    x = residual_block(x, 256)  # Apply the third residual block with 256 filters\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)  # Downsample again\n",
    "\n",
    "    # Add more residual blocks if the dataset is large and training time allows it\n",
    "    # x = residual_block(x, 512)\n",
    "    # x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Global Average Pooling (reduces parameter count while keeping spatial info)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Fully connected layer\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x) # Normalize the output\n",
    "    x = Dropout(0.7)(x) # Apply dropout for regularization to prevent overfitting\n",
    "\n",
    "    # Output layer for binary classification\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Define model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Model Training\n",
    "\n",
    "The training process involves:\n",
    "- **Data Augmentation**: We use techniques such as rotation, zoom and horizontal flip to artificially increase the size of our dataset and introduce variability.\n",
    "- **Class balancing**: There are fewer allowed speakers than disallowed, which is why class weights $w_c$ are applied during training to adjust the loss for each class.\n",
    "- **Optimization using the Adam algorithm**: The Adaptive Moment Estimation optimizer is used to minimize the loss function.\n",
    "- **Train/Test Split**: The dataset is split into 80% training and 20% validation using `train_test_split`.\n",
    "- **Early stopping**: The model's performance is monitored on the validation set, and training is stopped if the validation loss does not improve after $p = 15$ epochs.\n",
    "\n",
    "### Loading data for training: \n",
    "```python\n",
    "import os #for file and directory operations\n",
    "import sys #to handle system-specific parameters and functions\n",
    "import numpy as np #for efficient numerical computations\n",
    "import tensorflow as tf #a deep learning framework\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #for data augmentation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, TensorBoard #for training management\n",
    "from tensorflow.keras.optimizers import Adam #imports the Adam optimizer\n",
    "from model import create_cnn_model #Imports the CNN model architecture from another script/module\n",
    "from sklearn.utils.class_weight import compute_class_weight #to compute class weights for imbalanced datasets\n",
    "from sklearn.model_selection import train_test_split #to split data into training and testing sets\n",
    "from datetime import datetime  # For timestamping TensorBoard logs\n",
    "\n",
    "def load_data(spectrogram_dir, target_size=(128, 128)):\n",
    "    \"\"\"Load spectrogram images from the directory and assign labels based on subdirectories.\"\"\"\n",
    "    X, y = [], [] # Initialize empty lists to store images and labels\n",
    "    for class_name in ['allowed', 'disallowed']:  # Loop over classes\n",
    "        class_dir = os.path.join(spectrogram_dir, class_name)  # Path to each class folder\n",
    "        for file_name in os.listdir(class_dir): # Loop over files in the class folder\n",
    "            if file_name.endswith('_spectrogram.png'): # Only process spectrogram images\n",
    "                image_path = os.path.join(class_dir, file_name) # Full path to the image\n",
    "                # Load the image as grayscale with target dimensions and normalize it\n",
    "                image = tf.keras.preprocessing.image.load_img(image_path, color_mode='grayscale', target_size=target_size)\n",
    "                image_array = tf.keras.preprocessing.image.img_to_array(image) / 255.0 # Convert to array and normalize\n",
    "                X.append(image_array) # Append image data to X\n",
    "                y.append(1 if class_name == 'allowed' else 0) # Assign label 1 for \"allowed\", 0 for \"disallowed\"\n",
    "    return np.array(X), np.array(y) # Return images and labels as numpy arrays\n",
    "\n",
    "```\n",
    "\n",
    "### Training Code:\n",
    "\n",
    "```python\n",
    "def main():\n",
    "    # Load the data from the train directory (no separate test directory)\n",
    "    train_dir = './data/spectrograms/train'\n",
    "    X, y = load_data(train_dir) # Load training data and labels\n",
    "\n",
    "    # Convert labels to float32 type to avoid data type issues\n",
    "    y = y.astype(np.float32)\n",
    "\n",
    "    # Split data into 70% training and 30% testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Further split training data into training and validation (80% train, 20% validation from the 70%)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Compute class weights to handle class imbalance\n",
    "    class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    # Create the CNN model\n",
    "    model = create_cnn_model(input_shape=(128, 128, 1))\n",
    "    model.compile(optimizer=Adam(0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define ImageDataGenerator for augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "    # Ensure the data and labels are in correct format and shape\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "\n",
    "    # Use tf.data.Dataset to control the input pipeline structure explicitly\n",
    "    def data_generator(X, y, batch_size):\n",
    "        \"\"\"Generator to yield batches of data and labels as tf.float32.\"\"\"\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "        dataset = dataset.shuffle(len(y)).batch(batch_size)\n",
    "        dataset = dataset.map(lambda X, y: (tf.cast(X, tf.float32), tf.cast(y, tf.float32)))\n",
    "        return dataset\n",
    "\n",
    "    # Create train and validation datasets\n",
    "    train_dataset = data_generator(X_train, y_train, batch_size=32)\n",
    "    val_dataset = data_generator(X_val, y_val, batch_size=32)\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)  # Stops training if no improvement\n",
    "    checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True) # Saves the best model\n",
    "    lr_scheduler = LearningRateScheduler(lambda epoch, lr: lr * 0.9 if epoch > 10 else lr)# Decreases learning rate after epoch 10\n",
    "\n",
    "    # TensorBoard logging callback\n",
    "    log_dir = \"./logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1) # Logs training metrics to TensorBoard\n",
    "\n",
    "    # Train the model using the training and validation datasets\n",
    "    model.fit(\n",
    "        train_dataset,  # Training dataset\n",
    "        validation_data=val_dataset,  # Validation dataset\n",
    "        epochs=50, #number of training epochs\n",
    "        class_weight=class_weight_dict,  # Class weights to handle imbalance\n",
    "        callbacks=[early_stopping, checkpoint, lr_scheduler, tensorboard_callback]  # Added TensorBoard callback\n",
    "    )\n",
    "\n",
    "    model.save('final_model_2.keras') #save the final model\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_dataset = data_generator(X_test, y_test, batch_size=32)\n",
    "    test_loss, test_acc = model.evaluate(test_dataset) #Calculates test loss and accuracy\n",
    "    print(f\"Test accuracy: {test_acc}, Test loss: {test_loss}\") # Prints evaluation results\n",
    "\n",
    "# Main entry point to run the main function if the script is executed directly\n",
    "if __name__ == \"__main__\": \n",
    "    main()\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "1. **False Acceptance Ratio (FAR)**: Measures how often a disallowed person is incorrectly accepted.\n",
    "2. **False Rejection Ratio (FRR)**: Measures how often an allowed person is incorrectly rejected.\n",
    "3. **General Efficiency Coefficient**: A comprehensive metric that combines the model's accuracy, FAR, and FRR. The GEC is defined as:\n",
    "\n",
    "    $\\text{GEC} = w_1 \\cdot \\text{Accuracy} + w_2 \\cdot (1 - \\text{FAR}) + w_3 \\cdot (1 - \\text{FRR})$\n",
    "\n",
    "    where:\n",
    "    - $\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$\n",
    "    - $w_1 = 0.4$, $w_2 = 0.3$, and $w_3 = 0.3$ are the weights assigned to accuracy, $(1 - \\text{FAR})$, and $(1 - \\text{FRR})$ respectively, reflecting their relative importance in the overall evaluation.\n",
    "\n",
    "### FAR/FRR Calculation Code:\n",
    "\n",
    "```python\n",
    "def calculate_far_frr_binary(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate False Acceptance Ratio (FAR) and False Rejection Ratio (FRR) for binary classification.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Ground truth labels (1 for allowed, 0 for disallowed)\n",
    "    - y_pred: Model predictions (1 for allowed, 0 for disallowed)\n",
    "    \n",
    "    Returns:\n",
    "    - FAR: False Acceptance Ratio (disallowed incorrectly classified as allowed)\n",
    "    - FRR: False Rejection Ratio (allowed incorrectly classified as disallowed)\n",
    "    - tn: True negatives count\n",
    "    - fp: False positives count\n",
    "    - fn: False negatives count\n",
    "    - tp: True positives count\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    far = fp / (fp + tn) if (fp + tn) != 0 else 0  # False Acceptance Ratio\n",
    "    frr = fn / (fn + tp) if (fn + tp) != 0 else 0  # False Rejection Ratio\n",
    "\n",
    "    return far, frr, tn, fp, fn, tp\n",
    "```\n",
    "\n",
    "### GEC Calcultion Code: \n",
    "```python\n",
    "def calculate_general_efficiency_coefficient(accuracy, far, frr, w1=0.4, w2=0.3, w3=0.3):\n",
    "\"\"\"\n",
    "Calculate the General Efficiency Coefficient (GEC) as a weighted average of accuracy, (1 - FAR), and (1 - FRR).\n",
    "\n",
    "Parameters:\n",
    "- accuracy: Overall accuracy of the model\n",
    "- far: False Acceptance Ratio\n",
    "- frr: False Rejection Ratio\n",
    "- w1, w2, w3: Weights for accuracy, (1 - FAR), and (1 - FRR), respectively\n",
    "\n",
    "Returns:\n",
    "- gec: Calculated General Efficiency Coefficient\n",
    "\"\"\"\n",
    "gec = w1 * accuracy + w2 * (1 - far) + w3 * (1 - frr)\n",
    "return gec\n",
    "```\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "## Future Improvements\n",
    "\n",
    "Some ideas for further improvements:\n",
    "\n",
    "1. **Enhance Data Augmentation**: Apply more sophisticated augmentations to increase model robustness.\n",
    "2. **Advanced CNN Architectures**: Implement more advanced architectures like ResNet or VGG to improve performance.\n",
    "3. **Noise Handling**: Explore techniques for handling background noise more effectively, such as noise reduction or filtering.\n",
    "4. **Hyperparameter Tuning**: Experiment with different optimizers, learning rates, and batch sizes to improve model accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project demonstrates the application of **Convolutional Neural Networks** for voice recognition in a door-access system. By converting voice recordings into spectrograms, we leverage image-based recognition techniques to classify users based on their voice. The system shows promise and can be further improved with more sophisticated models and techniques.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d5cc8-0fd1-48fc-a816-5f7ce4417510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
